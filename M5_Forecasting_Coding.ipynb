{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='data'>1. Data Objective</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T05:27:30.110883Z",
     "start_time": "2020-07-09T05:27:30.094876Z"
    }
   },
   "source": [
    "The objective of the M5 forecasting competition is to advance the theory and practice of forecasting by identifying the method(s) that provide the most accurate point forecasts for each of the 42,840 time series of the competition. I addition, to elicit information to estimate the uncertainty distribution of the realized values of these series as precisely as possible. \n",
    "\n",
    "To that end, the participants of M5 are asked to provide 28 days ahead point forecasts (PFs) for all the series of the competition, as well as the corresponding median and 50%, 67%, 95%, and 99% prediction intervals (PIs).\n",
    "The M5 differs from the previous four ones in five important ways, some of them suggested by the discussants of the M4  competition, as follows:\n",
    "\n",
    "- First, it uses grouped unit sales data, starting at the product-store level and being aggregated to that of product departments, product categories, stores, and three geographical areas: the States of California (CA), Texas (TX), and Wisconsin (WI).\n",
    "\n",
    "- Second, besides the time series data, it includes explanatory variables such as sell prices, promotions, days of the week, and special events (e.g. Super Bowl, Valentine’s Day, and Orthodox Easter) that typically affect unit sales and could improve forecasting accuracy.\n",
    "\n",
    "- Third, in addition to point forecasts, it assesses the distribution of uncertainty, as the participants are asked to provide information on nine indicative quantiles.\n",
    "\n",
    "- Fourth, instead of having a single competition to estimate both the point forecasts and the uncertainty distribution, there will be two parallel tracks using the same dataset, the first requiring 28 days ahead point forecasts and the second 28 days ahead probabilistic forecasts for the median and four prediction intervals (50%, 67%, 95%, and 99%).\n",
    "\n",
    "- Fifth, for the first time it focuses on series that display intermittency, i.e., sporadic demand including zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dates and hosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The M5 will start on March 2, 2020 and finish on June 30 of the same year. The competition will be run using the Kaggle platform. Thus, we expect many submissions from all types of forecasters including data scientists, statisticians, and practitioners, expanding the field of forecasting and eventually integrating its various approaches for improving accuracy and uncertainty estimation.\n",
    "\n",
    "The competition will be divided into two separate Kaggle competitions, using the same dataset, with the first (M5 Forecasting Competition – Accuracy) requiring 28 days ahead point forecasts and the second (M5 Forecasting Competition – Uncertainty) 28 days ahead probabilistic forecasts for the corresponding median and four prediction intervals (50%, 67%, 95%, and 99%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI). In this respect, the bottom-level of the hierarchy, i.e., product-store unit sales can be mapped across either product categories or geographical regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Picture1.png](./Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The historical data range from 2011-01-29 to 2016-06-19. Thus, the products have a (maximum) selling history of 1,941  days / 5.4 years (test data of h=28 days not included). \n",
    "\n",
    "### The M5 dataset consists of the following four (4) files:\n",
    "\n",
    "### File 1: “calendar.csv” \n",
    "Contains information about the dates the products are sold.\n",
    "- date: The date in a “y-m-d” format.\n",
    "- wm_yr_wk: The id of the week the date belongs to.\n",
    "- weekday: The type of the day (Saturday, Sunday, …, Friday).\n",
    "- wday: The id of the weekday, starting from Saturday.\n",
    "- month: The month of the date.\n",
    "- year: The year of the date.\n",
    "- event_name_1: If the date includes an event, the name of this event.\n",
    "- event_type_1: If the date includes an event, the type of this event.\n",
    "- event_name_2: If the date includes a second event, the name of this event.\n",
    "- event_type_2: If the date includes a second event, the type of this event.\n",
    "- snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP  purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n",
    "\n",
    "### File 2: “sell_prices.csv”\n",
    "Contains information about the price of the products sold per store and date.\n",
    "- store_id: The id of the store where the product is sold. \n",
    "- item_id: The id of the product.\n",
    "- wm_yr_wk: The id of the week.\n",
    "- sell_price: The price of the product for the given week/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).  \n",
    "\n",
    "### File 3: “sales_train_valid.csv” \n",
    "Contains the historical daily unit sales data per product and store.\n",
    "- item_id: The id of the product.\n",
    "- dept_id: The id of the department the product belongs to.\n",
    "- cat_id: The id of the category the product belongs to.\n",
    "- store_id: The id of the store where the product is sold.\n",
    "- state_id: The State where the store is located.\n",
    "- d_1, d_2, …, d_i, … d_1941: The number of units sold at day i, starting from 2011-01-29.\n",
    "\n",
    "### File 4: “sales_train_evaluation.csv” \n",
    "Contains the historical daily unit sales data per product and store.\n",
    "- item_id: The id of the product.\n",
    "- dept_id: The id of the department the product belongs to.\n",
    "- cat_id: The id of the category the product belongs to.\n",
    "- store_id: The id of the store where the product is sold.\n",
    "- state_id: The State where the store is located.\n",
    "- d_1, d_2, …, d_i, … d_1969: The number of units sold at day i, starting from 2011-01-29. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='fetch'>2. Fetch the data</a>\n",
    "### 2.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:05:53.644042Z",
     "start_time": "2020-07-17T05:05:47.922638Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "for dirname, _, filenames in os.walk('data/m5-forecasting-accuracy/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T05:38:50.268545Z",
     "start_time": "2020-07-09T05:38:50.264556Z"
    }
   },
   "source": [
    "### 2.2. Setting working directory and loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:05:53.659042Z",
     "start_time": "2020-07-17T05:05:53.646042Z"
    }
   },
   "outputs": [],
   "source": [
    "# working path\n",
    "path = 'data/m5-forecasting-accuracy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:06:00.033061Z",
     "start_time": "2020-07-17T05:05:53.662032Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "calendar = pd.read_csv(path + 'calendar.csv')\n",
    "sellPrice = pd.read_csv(path + 'sell_prices.csv')\n",
    "sales = pd.read_csv(path + 'sales_train_evaluation.csv')\n",
    "sampleSubmission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:06:00.048075Z",
     "start_time": "2020-07-17T05:06:00.035085Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Calendar dataset has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "print('Sell Price dataset has {} rows and {} columns'.format(sellPrice.shape[0], sellPrice.shape[1]))\n",
    "print('Sales dataset has {} rows and {} columns'.format(sales.shape[0], sales.shape[1]))\n",
    "print('Sample Submission dataset has {} rows and {} columns'.format(sampleSubmission.shape[0], sampleSubmission.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:06:00.064075Z",
     "start_time": "2020-07-17T05:06:00.049076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function using to get number of rows and data types\n",
    "def infoData(df):\n",
    "    columns = []\n",
    "    values = []\n",
    "    for feature in df.columns:\n",
    "        columns.append(feature)\n",
    "        values.append(len(df[feature].unique()))\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    data = {'Unique_Value': values,'Percent_Missing': percent_missing, 'Data_Types': df.dtypes.values}\n",
    "    return pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:06:00.079087Z",
     "start_time": "2020-07-17T05:06:00.066081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Downcast in order to save memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:07:32.219095Z",
     "start_time": "2020-07-17T05:06:00.081078Z"
    }
   },
   "outputs": [],
   "source": [
    "# reducing memory of datasets\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "sellPrice = reduce_mem_usage(sellPrice)\n",
    "sales = reduce_mem_usage(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:07:32.250094Z",
     "start_time": "2020-07-17T05:07:32.221078Z"
    }
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:07:32.265095Z",
     "start_time": "2020-07-17T05:07:32.253088Z"
    }
   },
   "outputs": [],
   "source": [
    "sellPrice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:07:32.498075Z",
     "start_time": "2020-07-17T05:07:32.267081Z"
    }
   },
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data is now from 1914 to 1941. And test data is from 1942 to 1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:07:32.621071Z",
     "start_time": "2020-07-17T05:07:32.500095Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding more days to use as test datasets\n",
    "for d in range(1942, 1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales[col] = 0\n",
    "    sales[col] = sales[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:07:32.775067Z",
     "start_time": "2020-07-17T05:07:32.622073Z"
    }
   },
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='melt'>3. Melting the data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case what the melt function is doing is that it is converting the sales dataframe which is in wide format to a long format. I have kept the id variables as id, item_id, dept_id, cat_id, store_id and state_id. They have in total 30490 unique values when compunded together. Now the total number of days for which we have the data is 1969 days. Therefore the melted dataframe will be having 30490x1969 i.e. 60034810 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:08:23.106183Z",
     "start_time": "2020-07-17T05:07:32.777066Z"
    }
   },
   "outputs": [],
   "source": [
    "identifierVariables = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "variableName = 'd'\n",
    "measuredVariables = 'sales'\n",
    "\n",
    "data = pd.melt(sales,\n",
    "               id_vars = identifierVariables,\n",
    "               var_name = variableName,\n",
    "               value_name = measuredVariables).drop_duplicates()\n",
    "\n",
    "del identifierVariables, variableName, measuredVariables\n",
    "gc.collect()\n",
    "print('Dataset has {} rows and {} columns'.format(data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:08:23.122181Z",
     "start_time": "2020-07-17T05:08:23.108184Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine price data from prices dataframe and days data from calendar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:09:18.192833Z",
     "start_time": "2020-07-17T05:08:23.126181Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.merge(calendar, how='left', on='d')\n",
    "data = data.merge(sellPrice, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "\n",
    "del calendar, sellPrice\n",
    "gc.collect()\n",
    "print('Dataset has {} rows and {} columns'.format(data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:09:18.255528Z",
     "start_time": "2020-07-17T05:09:18.195814Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='eda'>4. Exploratory Data Analysis</a>\n",
    "### 4.1 The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:09:20.023273Z",
     "start_time": "2020-07-17T05:09:18.257501Z"
    }
   },
   "outputs": [],
   "source": [
    "group = sales.groupby(['state_id','store_id','cat_id','dept_id'],as_index=False)['item_id'].count().dropna()\n",
    "group['USA'] = 'United States of America'\n",
    "group.rename(columns={'state_id':'State','store_id':'Store','cat_id':'Category','dept_id':'Department','item_id':'Count'}, \n",
    "             inplace=True)\n",
    "fig = px.treemap(group, path=['USA', 'State', 'Store', 'Category', 'Department'], values='Count',\n",
    "                  color='Count',\n",
    "                  color_continuous_scale= px.colors.sequential.Sunset,\n",
    "                  title='Walmart: Distribution of items')\n",
    "fig.update_layout(template='seaborn')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Item Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:09:52.950294Z",
     "start_time": "2020-07-17T05:09:20.024269Z"
    }
   },
   "outputs": [],
   "source": [
    "group_price_store = data.groupby(['state_id','store_id','item_id'], as_index=False)['sell_price'].mean().dropna()\n",
    "fig = px.violin(group_price_store, x='store_id', color='state_id', y='sell_price', box=True, hover_name='item_id')\n",
    "fig.update_xaxes(title_text='Store')\n",
    "fig.update_yaxes(title_text='Selling Price($)')\n",
    "fig.update_layout(template='seaborn', title='Distribution of Items prices wrt Stores', legend_title_text='State')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:01.157348Z",
     "start_time": "2020-07-17T05:09:52.953292Z"
    }
   },
   "outputs": [],
   "source": [
    "group_price_cat = data.groupby(['store_id', 'cat_id', 'item_id'],as_index=False)['sell_price'].mean().dropna()\n",
    "fig = px.violin(group_price_cat, x='store_id', color='cat_id', y='sell_price', box=True, hover_name='item_id')\n",
    "fig.update_xaxes(title_text='Store')\n",
    "fig.update_yaxes(title_text='Selling Price($)')\n",
    "fig.update_layout(template='seaborn',title='Distribution of Items prices wrt Stores across Categories',\n",
    "                 legend_title_text='Category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the plot above, food category items are quite cheap as compared with hobbies and household items. Hobbies and household items have almost the same price range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Items Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:08.086329Z",
     "start_time": "2020-07-17T05:10:01.159326Z"
    }
   },
   "outputs": [],
   "source": [
    "group = data.groupby(['year','date', 'state_id', 'store_id'], as_index=False)['sales'].sum().dropna()\n",
    "fig = px.violin(group, x='store_id', y='sales', box=True)\n",
    "fig.update_xaxes(title_text='Store')\n",
    "fig.update_yaxes(title_text='Total items sales')\n",
    "fig.update_layout(template='seaborn', title='Distribution of Items Sales wrt Stores', legend_title_text='State')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- California: CA_3 has sold the most number of items while, CA_4 has sold the least number of items.\n",
    "- Texas: TX_2 and TX_3 have sold the maximum number of items. TX_1 has sold the least number of items.\n",
    "- Wisconsin: WI_2 has sold the maximum number of items while, WI_3 has sold the least number of items.\n",
    "- USA: CA_3 has sold the most number of items while, CA_4 has sold the least number of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:08.613338Z",
     "start_time": "2020-07-17T05:10:08.091344Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "title = 'Items Sales Over Time'\n",
    "years = group.year.unique().tolist()\n",
    "buttons = []\n",
    "y=3\n",
    "for state in group.state_id.unique().tolist():\n",
    "    group_state = group[group['state_id']==state]\n",
    "    for store in group_state.store_id.unique().tolist():\n",
    "        group_state_store = group_state[group_state['store_id']==store]\n",
    "        fig.add_trace(go.Scatter(name=store, x=group_state_store['date'], y=group_state_store['sales'], showlegend=True, \n",
    "                                   yaxis='y'+str(y) if y!=1 else 'y'))\n",
    "    y-=1\n",
    "\n",
    "fig.update_layout(\n",
    "        xaxis=dict(\n",
    "        #autorange=True,\n",
    "        range = ['2011-01-29','2016-05-22'],\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label=\"1m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=6,\n",
    "                     label=\"6m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=1,\n",
    "                     label=\"YTD\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"todate\"),\n",
    "                dict(count=1,\n",
    "                     label=\"1y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=2,\n",
    "                     label=\"2y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=3,\n",
    "                     label=\"3y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=4,\n",
    "                     label=\"4y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            autorange=True,\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        anchor=\"x\",\n",
    "        autorange=True,\n",
    "        domain=[0, 0.33],\n",
    "        mirror=True,\n",
    "        showline=True,\n",
    "        side=\"left\",\n",
    "        tickfont={\"size\":10},\n",
    "        tickmode=\"auto\",\n",
    "        ticks=\"\",\n",
    "        title='WI',\n",
    "        titlefont={\"size\":20},\n",
    "        type=\"linear\",\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        anchor=\"x\",\n",
    "        autorange=True,\n",
    "        domain=[0.33, 0.66],\n",
    "        mirror=True,\n",
    "        showline=True,\n",
    "        side=\"left\",\n",
    "        tickfont={\"size\":10},\n",
    "        tickmode=\"auto\",\n",
    "        ticks=\"\",\n",
    "        title = 'TX',\n",
    "        titlefont={\"size\":20},\n",
    "        type=\"linear\",\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        anchor=\"x\",\n",
    "        autorange=True,\n",
    "        domain=[0.66, 1],\n",
    "        mirror=True,\n",
    "        showline=True,\n",
    "        side=\"left\",\n",
    "        tickfont={\"size\":10},\n",
    "        tickmode=\"auto\",\n",
    "        ticks='',\n",
    "        title=\"CA\",\n",
    "        titlefont={\"size\":20},\n",
    "        type=\"linear\",\n",
    "        zeroline=False\n",
    "    )\n",
    "    )\n",
    "fig.update_layout(template='seaborn', title=title)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 State wise Analysis\n",
    "\n",
    "Let's see sales and revenue of all the stores individually across all the three states: California, Texas & Wisconsin. Plotting total three plots for each store: CA_1, CA_2, CA_3, CA_4, TX_1, TX_2, TX_3, WI_1, WI_2 & WI_3. Details about the plots are as follows:\n",
    "\n",
    "- First plot shows the daily sales of a store, plotting the values separately for SNAP days. Also, SNAP promotes food purchase, and food sales as well to check if it really affects the food sales.\n",
    "- Second plot shows the daily revenue of a store with separate plotting for <a href='#SNAP'>SNAP</a> days.\n",
    "- Third is a heatmap to show daily sales. It's plotted in such a way that it becomes easier to see day wise values.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<a id='SNAP'><b>What is SNAP?</b></a><br>\n",
    "The United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP).  SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products.  In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1/10 of the people will receive the benefit on their card.  More information about the SNAP program can be found [here.](https://www.fns.usda.gov/snap/supplemental-nutrition-assistance-program)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "For the heatmaps, the data is till 16th week of 2016 and datetime.weekofyear of function is returning 1,2 & 3 january of 2016 in 53rd week. Plotly's heatmap is connecting the data gap between the 16th and 53rd week. Still figuring out on how to remove this gap.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:08.861344Z",
     "start_time": "2020-07-17T05:10:08.615333Z"
    }
   },
   "outputs": [],
   "source": [
    "data['revenue'] = data['sales'] * data['sell_price'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:08.892332Z",
     "start_time": "2020-07-17T05:10:08.867350Z"
    }
   },
   "outputs": [],
   "source": [
    "def introduce_nulls(df):\n",
    "    idx = pd.date_range(df.date.dt.date.min(), df.date.dt.date.max())\n",
    "    df = df.set_index('date')\n",
    "    df = df.reindex(idx)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index':'date'},inplace=True)\n",
    "    return df\n",
    "\n",
    "def plot_metric(df,state,store,metric):\n",
    "    store_sales = df[(df['state_id']==state)&(df['store_id']==store)&(df['date']<='2016-05-22')]\n",
    "    food_sales = store_sales[store_sales['cat_id']=='FOODS']\n",
    "    store_sales = store_sales.groupby(['date','snap_'+state],as_index=False)['sales', 'revenue'].sum()\n",
    "    snap_sales = store_sales[store_sales['snap_'+state]==1]\n",
    "    non_snap_sales = store_sales[store_sales['snap_'+state]==0]\n",
    "    food_sales = food_sales.groupby(['date','snap_'+state],as_index=False)['sales', 'revenue'].sum()\n",
    "    snap_foods = food_sales[food_sales['snap_'+state]==1]\n",
    "    non_snap_foods = food_sales[food_sales['snap_'+state]==0]\n",
    "    non_snap_sales = introduce_nulls(non_snap_sales)\n",
    "    snap_sales = introduce_nulls(snap_sales)\n",
    "    non_snap_foods = introduce_nulls(non_snap_foods)\n",
    "    snap_foods = introduce_nulls(snap_foods)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=non_snap_sales['date'],y=non_snap_sales[metric],\n",
    "                           name='Total '+metric+'(Non-SNAP)'))\n",
    "    fig.add_trace(go.Scatter(x=snap_sales['date'],y=snap_sales[metric],\n",
    "                           name='Total '+metric+'(SNAP)'))\n",
    "    fig.add_trace(go.Scatter(x=non_snap_foods['date'],y=non_snap_foods[metric],\n",
    "                           name='Food '+metric+'(Non-SNAP)'))\n",
    "    fig.add_trace(go.Scatter(x=snap_foods['date'],y=snap_foods[metric],\n",
    "                           name='Food '+metric+'(SNAP)'))\n",
    "    fig.update_yaxes(title_text='Total items sold' if metric=='sold' else 'Total revenue($)')\n",
    "    fig.update_layout(template='seaborn',title=store)\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "        #autorange=True,\n",
    "        range = ['2011-01-29','2016-05-22'],\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label=\"1m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=6,\n",
    "                     label=\"6m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=1,\n",
    "                     label=\"YTD\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"todate\"),\n",
    "                dict(count=1,\n",
    "                     label=\"1y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=2,\n",
    "                     label=\"2y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=3,\n",
    "                     label=\"3y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=4,\n",
    "                     label=\"4y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            autorange=True,\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    ))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:08.923343Z",
     "start_time": "2020-07-17T05:10:08.894326Z"
    }
   },
   "outputs": [],
   "source": [
    "cal_data = group.copy()\n",
    "cal_data = cal_data[cal_data.date <= '22-05-2016']\n",
    "cal_data['week'] = cal_data.date.dt.weekofyear\n",
    "cal_data['day_name'] = cal_data.date.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:08.938330Z",
     "start_time": "2020-07-17T05:10:08.924333Z"
    }
   },
   "outputs": [],
   "source": [
    "def calmap(cal_data, state, store, scale):\n",
    "    cal_data = cal_data[(cal_data['state_id']==state)&(cal_data['store_id']==store)]\n",
    "    years = cal_data.year.unique().tolist()\n",
    "    fig = make_subplots(rows=len(years),cols=1,shared_xaxes=True,vertical_spacing=0.005)\n",
    "    r=1\n",
    "    for year in years:\n",
    "        data = cal_data[cal_data['year']==year]\n",
    "        data = introduce_nulls(data)\n",
    "        fig.add_trace(go.Heatmap(\n",
    "            z=data.sales,\n",
    "            x=data.week,\n",
    "            y=data.day_name,\n",
    "            hovertext=data.date.dt.date,\n",
    "            coloraxis = \"coloraxis\",name=year,\n",
    "        ),r,1)\n",
    "        fig.update_yaxes(title_text=year,tickfont=dict(size=5),row = r,col = 1)\n",
    "        r+=1\n",
    "    fig.update_xaxes(range=[1,53],tickfont=dict(size=10), nticks=53)\n",
    "    fig.update_layout(coloraxis = {'colorscale':scale})\n",
    "    fig.update_layout(template='seaborn', title=store)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cal\" class=\"btn btn-primary btn-lg btn-block active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">California</a>\n",
    "<img src='https://www.pixel4k.com/wp-content/uploads/2018/09/san-francisco-california-cityscape-4k_1538070292.jpg'\n",
    "style=\"width:800px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:16.377173Z",
     "start_time": "2020-07-17T05:10:08.940329Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'CA','CA_1','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:23.116166Z",
     "start_time": "2020-07-17T05:10:16.378171Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'CA','CA_1','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:23.256169Z",
     "start_time": "2020-07-17T05:10:23.118173Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'CA', 'CA_1', 'magma')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA_2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:30.107187Z",
     "start_time": "2020-07-17T05:10:23.257171Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data, 'CA', 'CA_2', 'sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:36.860196Z",
     "start_time": "2020-07-17T05:10:30.109171Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data, 'CA', 'CA_2', 'revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:36.984168Z",
     "start_time": "2020-07-17T05:10:36.863172Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'CA', 'CA_2', 'magma')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T06:12:46.391508Z",
     "start_time": "2020-07-09T06:12:46.382500Z"
    }
   },
   "source": [
    "### CA_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:43.866174Z",
     "start_time": "2020-07-17T05:10:36.986174Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'CA','CA_3','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:50.627187Z",
     "start_time": "2020-07-17T05:10:43.867168Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'CA','CA_3','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:50.752172Z",
     "start_time": "2020-07-17T05:10:50.630171Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'CA', 'CA_3', 'magma')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:10:57.539166Z",
     "start_time": "2020-07-17T05:10:50.754170Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data, 'CA', 'CA_4', 'sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:04.386175Z",
     "start_time": "2020-07-17T05:10:57.541169Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'CA','CA_4','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:04.511171Z",
     "start_time": "2020-07-17T05:11:04.388172Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'CA', 'CA_4', 'magma')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tex\" class=\"btn btn-primary btn-lg btn-block active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Texas</a>\n",
    "<img src='https://wallpaperaccess.com/full/227248.jpg'\n",
    "style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T06:20:06.068159Z",
     "start_time": "2020-07-09T06:20:06.058157Z"
    }
   },
   "source": [
    "### TX_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:11.518179Z",
     "start_time": "2020-07-17T05:11:04.513172Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'TX','TX_1','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:18.284168Z",
     "start_time": "2020-07-17T05:11:11.520176Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'TX','TX_1','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:18.408178Z",
     "start_time": "2020-07-17T05:11:18.286168Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'TX', 'TX_1', 'viridis')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TX_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:25.361689Z",
     "start_time": "2020-07-17T05:11:18.410169Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'TX','TX_2','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:32.157652Z",
     "start_time": "2020-07-17T05:11:25.365673Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'TX','TX_2','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:32.281660Z",
     "start_time": "2020-07-17T05:11:32.158653Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'TX', 'TX_2', 'viridis')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TX_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:39.125654Z",
     "start_time": "2020-07-17T05:11:32.283664Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'TX','TX_3','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:45.920651Z",
     "start_time": "2020-07-17T05:11:39.127652Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'TX','TX_3','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:46.044663Z",
     "start_time": "2020-07-17T05:11:45.922652Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'TX', 'TX_3', 'viridis')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wis\" class=\"btn btn-primary btn-lg btn-block active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Wisconsin</a>\n",
    "<img src='https://i.ytimg.com/vi/RzETB_wVAKI/maxresdefault.jpg'\n",
    "style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WI_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:52.907654Z",
     "start_time": "2020-07-17T05:11:46.045661Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'WI','WI_1','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:59.726653Z",
     "start_time": "2020-07-17T05:11:52.909654Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'WI','WI_1','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:11:59.850660Z",
     "start_time": "2020-07-17T05:11:59.727654Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'WI', 'WI_1', 'twilight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T06:25:02.713965Z",
     "start_time": "2020-07-09T06:25:02.693960Z"
    }
   },
   "source": [
    "### WI_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:06.738656Z",
     "start_time": "2020-07-17T05:11:59.852670Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'WI','WI_2','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:13.507661Z",
     "start_time": "2020-07-17T05:12:06.739662Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data, 'WI','WI_2','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:13.631680Z",
     "start_time": "2020-07-17T05:12:13.508661Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'WI', 'WI_2', 'twilight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WI_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:20.488660Z",
     "start_time": "2020-07-17T05:12:13.633672Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'WI','WI_3','sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:27.534653Z",
     "start_time": "2020-07-17T05:12:20.489660Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_metric(data,'WI','WI_3','revenue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:27.658658Z",
     "start_time": "2020-07-17T05:12:27.535656Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = calmap(cal_data, 'WI', 'WI_3', 'twilight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:27.735650Z",
     "start_time": "2020-07-17T05:12:27.662659Z"
    }
   },
   "outputs": [],
   "source": [
    "del group, group_price_cat, group_price_store, group_state, group_state_store, cal_data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='fe'>5. Feature Engineering</a>\n",
    "\n",
    "<img src='https://media.giphy.com/media/yv1ggi3Cbase05a8iS/giphy.gif' style=\"width:500px;height:300px;\">\n",
    "\n",
    "The goal of feature engineering is to provide strong and ideally simple relationships between new input features and the output feature for the supervised learning algorithm to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Label Encoding\n",
    "<ol>\n",
    "  <li>Remove unwanted data to create space in RAM for further processing.</li>\n",
    "  <li>Label Encode categorical features.(I had converted already converted categorical variable to category type. So, I can simply use their codes instead of using LableEncoder)</li>\n",
    "  <li>Remove date as its features are already present.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:36.398660Z",
     "start_time": "2020-07-17T05:12:27.737663Z"
    }
   },
   "outputs": [],
   "source": [
    "nanFeatures = ['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2']\n",
    "for feature in nanFeatures:\n",
    "    data[feature].fillna('MISSING', inplace=True)\n",
    "    \n",
    "del nanFeatures\n",
    "gc.collect()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:14:14.242673Z",
     "start_time": "2020-07-17T05:12:36.399660Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = data.columns.drop([\n",
    "    'sales', 'date', 'sell_price', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI', 'id', 'd'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    data[feature] = le.fit_transform(data[feature])\n",
    "\n",
    "del categorical_features\n",
    "gc.collect()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Introduce Lags\n",
    "\n",
    "Lag features are the classical way that time series forecasting problems are transformed into supervised learning problems.\n",
    "\n",
    "Introduce lags to the the target variable `sales`. The maximum lag I have introduced is 30 days. It's purely upto you how many lags you want to introduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:17:28.655233Z",
     "start_time": "2020-07-17T05:14:14.245654Z"
    }
   },
   "outputs": [],
   "source": [
    "data['lag_t28'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28)).astype(np.float16)\n",
    "data['lag_t29'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(29)).astype(np.float16)\n",
    "data['lag_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(30)).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Mean and Median Encoding\n",
    "\n",
    "From a mathematical point of view, mean encoding represents a probability of your target variable, conditional on each value of the feature. In a way, it embodies the target variable in its encoded value. I have calculated mean encodings on the basis of following logical features I could think of:-\n",
    "- item\n",
    "- state\n",
    "- store\n",
    "- category\n",
    "- department\n",
    "- category & department\n",
    "- store & item\n",
    "- category & item\n",
    "- department & item\n",
    "- state & store\n",
    "- state, store and category\n",
    "- store, category and department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:18:16.631171Z",
     "start_time": "2020-07-17T05:17:28.657232Z"
    }
   },
   "outputs": [],
   "source": [
    "data['iteam_sold_avg_mean'] = data.groupby('item_id')['sales'].transform('mean').astype(np.float16)\n",
    "data['state_sold_avg_mean'] = data.groupby('state_id')['sales'].transform('mean').astype(np.float16)\n",
    "data['store_sold_avg_mean'] = data.groupby('store_id')['sales'].transform('mean').astype(np.float16)\n",
    "data['cat_sold_avg_mean'] = data.groupby('cat_id')['sales'].transform('mean').astype(np.float16)\n",
    "data['dept_sold_avg_mean'] = data.groupby('dept_id')['sales'].transform('mean').astype(np.float16)\n",
    "data['cat_dept_sold_avg_mean'] = data.groupby(['cat_id', 'dept_id'])['sales'].transform('mean').astype(np.float16)\n",
    "data['store_item_sold_avg_mean'] = data.groupby(['store_id', 'item_id'])['sales'].transform('mean').astype(np.float16)\n",
    "data['cat_item_sold_avg_mean'] = data.groupby(['cat_id', 'item_id'])['sales'].transform('mean').astype(np.float16)\n",
    "data['dept_item_sold_avg_mean'] = data.groupby(['dept_id', 'item_id'])['sales'].transform('mean').astype(np.float16)\n",
    "data['state_store_sold_avg_mean'] = data.groupby(['state_id', 'store_id'])['sales'].transform('mean').astype(np.float16)\n",
    "data['state_store_cat_sold_avg_mean'] = data.groupby(['state_id', 'store_id', 'cat_id'])['sales'].transform('mean').astype(np.float16)\n",
    "data['store_cat_dept_sold_avg_mean'] = data.groupby(['store_id', 'cat_id', 'dept_id'])['sales'].transform('mean').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:19:15.121337Z",
     "start_time": "2020-07-17T05:18:16.635159Z"
    }
   },
   "outputs": [],
   "source": [
    "data['iteam_sold_avg_median'] = data.groupby('item_id')['sales'].transform('median').astype(np.float16)\n",
    "data['state_sold_avg_median'] = data.groupby('state_id')['sales'].transform('median').astype(np.float16)\n",
    "data['store_sold_avg_median'] = data.groupby('store_id')['sales'].transform('median').astype(np.float16)\n",
    "data['cat_sold_avg_median'] = data.groupby('cat_id')['sales'].transform('median').astype(np.float16)\n",
    "data['dept_sold_avg_median'] = data.groupby('dept_id')['sales'].transform('median').astype(np.float16)\n",
    "data['cat_dept_sold_avg_median'] = data.groupby(['cat_id', 'dept_id'])['sales'].transform('median').astype(np.float16)\n",
    "data['store_item_sold_avg_median'] = data.groupby(['store_id', 'item_id'])['sales'].transform('median').astype(np.float16)\n",
    "data['cat_item_sold_avg_median'] = data.groupby(['cat_id', 'item_id'])['sales'].transform('median').astype(np.float16)\n",
    "data['dept_item_sold_avg_median'] = data.groupby(['dept_id', 'item_id'])['sales'].transform('median').astype(np.float16)\n",
    "data['state_store_sold_avg_median'] = data.groupby(['state_id', 'store_id'])['sales'].transform('median').astype(np.float16)\n",
    "data['state_store_cat_sold_avg_median'] = data.groupby(['state_id', 'store_id', 'cat_id'])['sales'].transform('median').astype(np.float16)\n",
    "data['store_cat_dept_sold_avg_median'] = data.groupby(['store_id', 'cat_id', 'dept_id'])['sales'].transform('median').astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T06:38:39.622313Z",
     "start_time": "2020-07-09T06:38:39.607315Z"
    }
   },
   "source": [
    "### 5.4 Rolling Window Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:26:44.476292Z",
     "start_time": "2020-07-17T05:19:15.125792Z"
    }
   },
   "outputs": [],
   "source": [
    "data['rolling_mean_t7'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).mean()).astype(np.float16)\n",
    "data['rolling_std_t7'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).std()).astype(np.float16)\n",
    "data['rolling_std_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).std()).astype(np.float16)\n",
    "data['rolling_mean_t30'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).mean()).astype(np.float16)\n",
    "data['rolling_mean_t90'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(90).mean()).astype(np.float16)\n",
    "data['rolling_mean_t180'] = data.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).mean()).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T06:39:44.619662Z",
     "start_time": "2020-07-09T06:39:44.614662Z"
    }
   },
   "source": [
    "### 5.5 Transform Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:28:31.618040Z",
     "start_time": "2020-07-17T05:26:44.487291Z"
    }
   },
   "outputs": [],
   "source": [
    "data['lag_price'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "data['price_change'] = (data['lag_price'] - data['sell_price']) / (data['lag_price']).astype(np.float16)\n",
    "data['price_std'] = (data['sell_price'] - data['sell_price'].min()) / (1 + data['sell_price'].max() - data['sell_price'].min())\n",
    "data['price_std'] = data['price_std'].astype(np.float16)\n",
    "data.drop(['lag_price'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T06:41:35.342731Z",
     "start_time": "2020-07-09T06:41:35.327725Z"
    }
   },
   "source": [
    "### 5.6 Adding daytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:28:55.525898Z",
     "start_time": "2020-07-17T05:28:31.638578Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(['weekday', 'wday', 'month', 'year', 'wm_yr_wk'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:29:26.198637Z",
     "start_time": "2020-07-17T05:28:55.535906Z"
    }
   },
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = (data['date'].dt.year - 2010).astype(np.int16)\n",
    "data['quarter'] = data['date'].dt.quarter.astype(np.int16)\n",
    "data['month'] = data['date'].dt.month.astype(np.int16)\n",
    "data['week'] = data['date'].dt.week.astype(np.int16)\n",
    "data['day'] = data['date'].dt.day.astype(np.int16)\n",
    "data['dayofweek'] = data['date'].dt.dayofweek.astype(np.int16)\n",
    "data['d'] = data['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'F7'><h2>5.7 Saving the data</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:30:02.116170Z",
     "start_time": "2020-07-17T05:29:26.200639Z"
    }
   },
   "outputs": [],
   "source": [
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:31:03.204250Z",
     "start_time": "2020-07-17T05:30:02.119174Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[data['d'] >= 180]\n",
    "data.to_pickle(path + \"cleaned_data.pkl\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred'><h2>6. Modelling and Prediction</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:31:03.281240Z",
     "start_time": "2020-07-17T05:31:03.254244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "params = {\n",
    "        \"metric\": \"rmse\",\n",
    "        \"objective\": \"poisson\",\n",
    "        \"alpha\": 0.1,\n",
    "        \"lambda\": 0.1,\n",
    "        \"seed\": 42,\n",
    "        \"num_leaves\": 100,\n",
    "        \"learning_rate\": 0.075,\n",
    "        \"bagging_fraction\": 0.75,\n",
    "        \"bagging_freq\": 2,\n",
    "        \"colsample_bytree\": 0.75,\n",
    "    }\n",
    "\n",
    "fit_params = {\n",
    "        \"num_boost_round\": 2000,\n",
    "        \"early_stopping_rounds\": 200,\n",
    "        \"verbose_eval\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:31:03.311641Z",
     "start_time": "2020-07-17T05:31:03.282244Z"
    }
   },
   "outputs": [],
   "source": [
    "features = data.columns.drop(['id', 'date', 'd', 'sales','revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:31:04.008691Z",
     "start_time": "2020-07-17T05:31:03.312644Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = data[(data['d'] >= 1914) & (data['d'] < 1942)][['id', 'd', 'sales']]\n",
    "test = data[data['d'] >= 1942][['id', 'd', 'sales']]\n",
    "eval_preds = test['sales']\n",
    "valid_preds = valid['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T06:06:35.510702Z",
     "start_time": "2020-07-17T05:31:04.010666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training individual store\n",
    "\n",
    "stores = data['store_id'].unique().tolist()\n",
    "for store in stores:\n",
    "    print(\"STORE TRAINING: \", store+1)\n",
    "    df = data[data['store_id'] == store]\n",
    "\n",
    "    #Split the data\n",
    "    X_train, y_train = df[df['d'] < 1914].drop('sales', axis=1), df[df['d'] < 1914]['sales']\n",
    "    X_valid, y_valid = df[(df['d'] >= 1914) & (df['d'] < 1942)].drop('sales', axis=1), df[(df['d'] >= 1914) & (df['d'] < 1942)]['sales']\n",
    "    X_test = df[df['d'] >= 1942].drop('sales', axis=1)\n",
    "    \n",
    "    train_set = lgb.Dataset(X_train[features], y_train)\n",
    "    valid_set = lgb.Dataset(X_valid[features], y_valid)\n",
    "\n",
    "    #Train and validate\n",
    "    model = lgb.train(params, train_set, valid_sets=[train_set, valid_set], **fit_params)\n",
    "    valid_rmse = np.sqrt(mean_squared_error(model.predict(X_valid[features]), y_valid))\n",
    "    eval_preds[X_test[features].index] = model.predict(X_test[features])\n",
    "\n",
    "    del model, X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T06:06:38.292593Z",
     "start_time": "2020-07-17T06:06:35.513702Z"
    }
   },
   "outputs": [],
   "source": [
    "validation = sales[['id'] + ['d_' + str(i) for i in range(1914, 1942)]]\n",
    "validation['id'] = pd.read_csv(path + 'sales_train_validation.csv').id\n",
    "validation.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T06:06:38.837391Z",
     "start_time": "2020-07-17T06:06:38.294593Z"
    }
   },
   "outputs": [],
   "source": [
    "test['sales'] = eval_preds\n",
    "predictions = test[['id', 'd', 'sales']]\n",
    "predictions = pd.pivot(predictions, index='id', columns='d', values='sales').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "evaluation = sampleSubmission[['id']].merge(predictions, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T06:06:40.582741Z",
     "start_time": "2020-07-17T06:06:38.839369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the submission\n",
    "submit = pd.concat([validation, evaluation]).reset_index(drop=True)\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit[30490:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='con'><h2>7. Conclusion</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one model LightGBM and those features above, we can get 0.54928 private score and top 12 on M5 forecasting accuracy competition of Kaggle leader board.\n",
    "\n",
    "![Capture.png](./Capture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Work**\n",
    "- Focus more in feature engineering part\n",
    "- Hyperparameter tunning\n",
    "- Ensembling models Lasso, Ridge and LightGBM (certainly better score)\n",
    "- Deploying categorical embedding\n",
    "- Deploying LSTM model\n",
    "\n",
    "**Reference**\n",
    "- EDA reference\n",
    "\n",
    "https://www.kaggle.com/anshuls235/time-series-forecasting-eda-fe-modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
